== EBike Application in Event Driven Architecture


The Domain-Driven Design (DDD) architecture is not repeated here, since it's already provided in the report of the second assignment, available at link:https://github.com/stormtroober/microservices-ebikes/blob/main/doc/asciidoc/doc/assets/docs/report.pdf[Assignment 2 Report].

For transforming the EBike application into an event-driven architecture, Kafka is used as the event broker, enabling asynchronous communication between the microservices. This section describes the Kafka topics used, the event flow between services, and the configuration of adapters that produce and consume events.

=== Kafka Topics
This part describes the Kafka topics used to transform the EBike original architecture into an event-driven architecture. The topics are designed to facilitate communication between the ebike, ride, map, and user microservices, ensuring that each service can react to relevant events without direct dependencies on one another.

**Topics Used**:

- **ebike-updates**: This topic carries status and position updates from ebikes to the map service (for visualization purposes) and to the ride service (for local state synchronization). When an ebike changes position or updates its status (available, in use, maintenance), these events are published here.

- **ebike-ride-update**: This topic handles ride-related updates that flow from the ride service to the ebike service. It includes events like ride start/end notifications that trigger state changes in the bicycle's operational status.

- **ride-map-update**: This topic transmits ride events (start/end of rides) from the ride service to the map service. These updates ensure that the map visualization remains current with active rides and completed journeys.

- **ride-user-update**: This topic carries updates from the ride service to the user service, including credit charges for completed rides and ride status changes that affect the user experience.

- **user-update**: This topic handles general user data updates (such as profile information) from the user service to the ride service. These events are used to maintain synchronized local state across services when user information changes.

=== Event Flow

At the purpose of explaining the event flow, i'll detail how events are produced and consumed across the microservices involved in the EBike system.

==== Detailed Communication Patterns

[plantuml, {diagramsdir}/kafka-comm-ebike, svg, title="Microservices communications of EBike system using Kafka", width=60%]
----
!include resources/puml/kafka-comm-ebike.puml
----

===== 1. EBike State Update
- **Producer:** ebike-microservice (_MapCommunicationAdapter_)
- **Topic:** ebike-updates
- **Consumers:** map-microservice (_BikeUpdateAdapter_), ride-microservice (_BikeConsumerAdapter_)
- **Flow:** When an EBike's state or position changes, the ebike-microservice publishes this update to the ebike-updates topic. The map service consumes this message to update the bike's position on the map, while the ride service updates its local repository of available e-bikes.

===== 2. Ride Events Affecting EBike
- **Producer:** ride-microservice (_EBikeCommunicationAdapter_)
- **Topic:** ebike-ride-update
- **Consumer:** ebike-microservice (_RideCommunicationAdapter_)
- **Flow:** When the ride service processes a ride event (e.g., start/end ride), it publishes an update to the ebike-ride-update topic. The ebike service consumes this message and updates the EBike's state accordingly (e.g., from AVAILABLE to IN_USE).

===== 3. Ride Events Affecting Map
- **Producer:** ride-microservice (_MapCommunicationAdapter_)
- **Topic:** ride-map-update
- **Consumer:** map-microservice (_RideUpdateAdapter_)
- **Flow:** When the ride service processes a ride event, it sends a message to the ride-map-update topic. The map service consumes this message to associate or disassociate the user with the bike on the map.

===== 4. Ride Events Affecting User
- **Producer:** ride-microservice (_UserCommunicationAdapter_)
- **Topic:** ride-user-update
- **Consumer:** user-microservice (_RideConsumerAdapter_)
- **Flow:** When the ride service handles user-related events (e.g., charging for a completed ride), it sends an update to the ride-user-update topic. The user service consumes this message to update user data (e.g., remaining credit).

===== 5. User Data Update
- **Producer:** user-microservice (_RideProducerAdapter_)
- **Topic:** user-update
- **Consumer:** ride-microservice (_UserConsumerAdapter_)
- **Flow:** When user data changes in the user service, it sends an update to the user-update topic. The ride service consumes this message to keep its local user repository synchronized.

==== Ride Scenario

When a user starts a ride through the system, a complex sequence of events propagates through the microservices:

1. The ride service receives the API call to start a ride and becomes the initial event producer
2. The ride service publishes events to multiple topics:
   - To ebike-ride-update to inform the ebike service to mark the bike as in use
   - To ride-map-update to update the map visualization
   - To ride-user-update to charge the user's credit for the ride

3. The ebike service responds to these events by:
   - Consuming the ebike-ride-update message and changing the bike's status
   - Publishing its own update to ebike-updates to notify all interested services of the bike's new state

4. The map service maintains an up-to-date view by:
   - Consuming ebike-updates to have current bike positions and statuses
   - Consuming ride-map-update to visualize user-bike associations

5. The user service consumes ride-user-update messages to manage user credit and ride history

=== Event Sourcing Implementation

The User Microservice has been choosen for event sourcing because it handles critical user data, such as credit and ride history, which must be accurately tracked and auditable. By storing events in a MongoDB collection, the system can maintain a complete history of user interactions, enabling features like user activity tracking and credit management.

==== Event Types and Structure

The event sourcing implementation defines three main event types that capture all possible state changes for a user:

.User Event Types
[source,java]
----
public enum UserEventType {
  USER_CREATED("UserCreated"),
  CREDIT_UPDATED("CreditUpdated"),
  CREDIT_RECHARGED("CreditRecharged");
  // ...existing code...
}
----

Each event implements the `UserEvent` interface, which provides common properties for event identification and ordering:

.UserEvent Interface
[source,java]
----
public interface UserEvent {
  String getAggregateId(); // username
  long getSequence();      // version for ordering
  long getOccurredAt();    // timestamp
  UserEventType getType(); // event type identifier
}
----

The `UserCreated` event captures the initial user registration with their type and starting credit:

.UserCreated Event Structure
[source,java]
----
public final class UserCreated implements UserEvent {
  private final String aggregateId;
  private final long sequence;
  private final UserEventType type = UserEventType.USER_CREATED;
  
  // Payload fields
  private final String userType;
  private final int initialCredit;
  
  public UserCreated(String aggregateId, long sequence, String userType, int initialCredit) {
    this.aggregateId = aggregateId;
    this.sequence = sequence;
    this.userType = userType;
    this.initialCredit = initialCredit;
    this.occurredAt = System.currentTimeMillis();
  }
  // ...existing code...
}
----

==== User Aggregate Pattern

The `UserAggregate` class implements the aggregate pattern, maintaining the current state of a user by applying a sequence of events. This approach ensures that the current state can always be reconstructed from the event history:

.User Aggregate State Reconstruction
[source,java]
----
public class UserAggregate {
  private String username;
  private String userType;
  private int credit;
  private long version;

  public UserAggregate(List<UserEvent> history) {
    this.version = 0;
    history.forEach(this::applyEvent);
  }

  public void applyEvent(UserEvent evt) {
    switch (evt.getType()) {
      case USER_CREATED:
        UserCreated uc = (UserCreated) evt;
        this.username = uc.getAggregateId();
        this.userType = uc.getUserType();
        this.credit = uc.getInitialCredit();
        break;
      case CREDIT_RECHARGED:
        this.credit += ((CreditRecharged) evt).getAmount();
        break;
      case CREDIT_UPDATED:
        this.credit = ((CreditUpdated) evt).getNewCredit();
        break;
      // ...existing code...
    }
    this.version = evt.getSequence();
  }
  // ...existing code...
}
----

The aggregate also provides command methods that generate new events while enforcing business rules:

.Command Methods in UserAggregate
[source,java]
----
public UserCreated create(String username, String userType, int initialCredit) {
  if (version != 0) throw new IllegalStateException("Already created");
  return new UserCreated(username, version + 1, userType, initialCredit);
}

public CreditUpdated updateCredit(int newCredit) {
  if (version == 0) throw new IllegalStateException("Not created");
  return new CreditUpdated(username, version + 1, newCredit);
}
----

==== MongoDB Event Store

The `MongoEventStore` provides persistent storage for events with proper serialization and deserialization. Events are stored as JSON documents with a structured format that includes metadata and payload:

.Event Storage Structure
[source,java]
----
JsonObject doc = new JsonObject()
    .put("aggregateId", aggregateId)
    .put("sequence", event.getSequence())
    .put("type", event.getType().getValue())
    .put("occurredAt", event.getOccurredAt())
    .put("payload", payload);
----

The event store supports loading events by aggregate ID and sequence number, enabling efficient state reconstruction:

.Event Loading with Ordering
[source,java]
----
JsonObject query = new JsonObject()
    .put("aggregateId", aggregateId)
    .put("sequence", new JsonObject().put("$gte", fromSequence));

// Sort by sequence ascending for proper event ordering
results.sort(Comparator.comparingInt(d -> d.getInteger("sequence")));
----

==== Event-Sourced Service Implementation

The `UserServiceEventSourcedImpl` coordinates between the aggregate, event store, and external publishers. It maintains an in-memory cache of aggregates for performance while ensuring consistency through the event store:

.Aggregate Caching and Loading
[source,java]
----
private CompletableFuture<UserAggregate> getOrLoad(String username) {
  UserAggregate cached = cache.get(username);
  if (cached != null) {
    return CompletableFuture.completedFuture(cached);
  }

  return eventStore.loadEvents(username, 0)
      .thenApply(history -> {
        UserAggregate agg = new UserAggregate(history);
        cache.put(username, agg);
        return agg;
      });
}
----

The service follows a command-event-publish pattern for all operations:

.Command Processing Pattern
[source,java]
----
public CompletableFuture<JsonObject> updateCredit(String username, int newCredit) {
  return getOrLoad(username)
      .thenCompose(agg -> {
        // 1. Generate event from aggregate
        CreditUpdated evt = agg.updateCredit(newCredit);
        
        // 2. Persist event to store
        return eventStore.appendEvent(username, evt, agg.getVersion())
            .thenApply(v -> {
              // 3. Apply event to aggregate
              agg.applyEvent(evt);
              JsonObject userJson = agg.toJson();
              
              // 4. Publish to external systems
              userEventPublisher.publishUserUpdate(username, userJson);
              return userJson;
            });
      });
}
----

This pattern ensures that:

- Business logic is enforced through the aggregate
- All state changes are captured as events
- External systems are notified of changes
- The system can recover from any point in time by replaying events

.User-Events storage in MongoDB
[source,json]
----
{
  "_id": "684e9f94846dfb422934f045",
  "aggregateId": "ale",
  "sequence": {
    "$numberLong": "1"
  },
  "type": "UserCreated",
  "occurredAt": {
    "$numberLong": "1749983124244"
  },
  "payload": {
    "userType": "USER",
    "initialCredit": 100
  }
}
{
  "_id": "684e9f99846dfb422934f046",
  "aggregateId": "tone",
  "sequence": {
    "$numberLong": "1"
  },
  "type": "UserCreated",
  "occurredAt": {
    "$numberLong": "1749983129463"
  },
  "payload": {
    "userType": "ADMIN",
    "initialCredit": 100
  }
}
{
  "_id": "684e9ff7846dfb422934f047",
  "aggregateId": "ale",
  "sequence": {
    "$numberLong": "2"
  },
  "type": "CreditUpdated",
  "occurredAt": {
    "$numberLong": "1749983223195"
  },
  "payload": {
    "newCredit": 99
  }
}
----

=== Adapter Configuration

Every adapter uses a shared Kafka configuration to connect to the Kafka Cluster.

.Kafka Producer Configuration
[source,java]
----
public Properties getProducerProperties() {
    Properties props = new Properties();
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerAddress);
    props.put(ProducerConfig.ACKS_CONFIG, "all");
    props.put(ProducerConfig.RETRIES_CONFIG, 5);
    props.put(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG, 1000);
    props.put(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG, 5000);
    props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 500);
    props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
    props.put(ProducerConfig.LINGER_MS_CONFIG, 1);
    props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
    props.put(
        ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
        "org.apache.kafka.common.serialization.StringSerializer");
    props.put(
        ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
        "org.apache.kafka.common.serialization.StringSerializer");
    return props;
}
----



.Kafka Consumer Configuration
[source,java]
----
public Properties getConsumerProperties() {
    Properties props = new Properties();
    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerAddress);
    props.put(ConsumerConfig.GROUP_ID_CONFIG, "ebike-user-group");
    props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
    props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "30000");
    props.put(
            ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
            "org.apache.kafka.common.serialization.StringDeserializer");
    props.put(
            ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
            "org.apache.kafka.common.serialization.StringDeserializer");
    return props;
  }
----


The _Consumer_ adapters execute on a separate thread, managed through a single-thread `ExecutorService`. This approach allows for continuous background polling of Kafka messages without blocking the main thread. The polling cycle processes incoming messages by transforming them into JSON objects and updating the appropriate repository (e.g., user, bike, or ride repository depending on the adapter).

.Kafka Consumer Execution
[source,java]
----
private void startKafkaConsumer() {
    consumerExecutor = Executors.newSingleThreadExecutor();
    running.set(true);
    consumerExecutor.submit(this::runKafkaConsumer);
  }
----



=== Deployment Configuration

The EBike system uses Docker Compose to orchestrate its services, including the Kafka event streaming platform. The Kafka infrastructure consists of Zookeeper for coordination and a Kafka broker for message handling, both integrated into the application's network.

==== Kafka Infrastructure in Docker Compose

The following services are added to the Docker Compose configuration to support the event sourcing architecture:

- **Zookeeper**: Manages the Kafka cluster coordination
- **Kafka Broker**: Handles the message queuing and delivery
- **Redpanda Console**: Provides a web UI for monitoring Kafka topics and messages

.Docker Compose Configuration for Kafka
[source,yaml]
----
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - eureka-network

  kafka-broker:
    image: confluentinc/cp-kafka:5.5.0
    hostname: ${KAFKA_BROKER_HOSTNAME}
    depends_on:
      - zookeeper
    ports:
      - "${KAFKA_BROKER_EXTERNAL_PORT}:${KAFKA_BROKER_EXTERNAL_PORT}"
    networks:
      - eureka-network
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${KAFKA_BROKER_HOSTNAME}:${KAFKA_BROKER_PORT},PLAINTEXT_HOST://localhost:${KAFKA_BROKER_EXTERNAL_PORT}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    healthcheck:
      test: [ "CMD-SHELL", "kafka-topics --bootstrap-server localhost:${KAFKA_BROKER_EXTERNAL_PORT} --list || exit 1" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 45s

  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:latest
    ports:
      - "8087:8080"
    networks:
      - eureka-network
    environment:
      KAFKA_BROKERS: "kafka-broker:9092"
    depends_on:
      kafka-broker:
        condition: service_healthy
----

==== Environment Variables

The following environment variables are set in the `.env` file to configure the Kafka broker:

[source,properties]
----
#kafka configuration
KAFKA_BROKER_HOSTNAME=kafka-broker
KAFKA_BROKER_PORT=9092
KAFKA_BROKER_EXTERNAL_PORT=29092
----

These variables are referenced in the Docker Compose file and passed to each microservice to ensure consistent Kafka broker configuration across the system. The internal port (9092) is used for service-to-service communication within the Docker network, while the external port (29092) is mapped to the host for access from outside the container environment.

Each microservice container receives these Kafka connection parameters through environment variables, which are then used in their respective adapter configurations to establish producer and consumer connections to the Kafka broker.