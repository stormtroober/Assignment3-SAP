== Kubernetes Deployment Transformation

This section describes how the EBike event sourcing application was transformed from a Docker Compose-based deployment to a Kubernetes-native deployment, maintaining the same event-driven architecture while leveraging Kubernetes orchestration capabilities.

=== Overview of Transformation

The transformation from Docker Compose to Kubernetes involved several key adaptations:

- **Configuration Management**: Environment variables moved from `.env` files to Kubernetes ConfigMaps
- **Service Discovery**: Adapted Eureka service registration to work with Kubernetes DNS
- **Networking**: Replaced Docker bridge networks with Kubernetes Services and DNS resolution
- **Dependencies**: Converted Docker Compose `depends_on` to Kubernetes init containers
- **Storage**: Transformed Docker volumes into Kubernetes PersistentVolumeClaims
- **Orchestration**: Replaced Docker Compose services with Kubernetes Deployments and Services

=== Configuration Management Migration

==== From Environment Files to ConfigMaps

The Docker Compose deployment used `.env` files for configuration management:

.Docker Compose Environment Configuration
[source,properties]
----
#kafka configuration
KAFKA_BROKER_HOSTNAME=kafka-broker
KAFKA_BROKER_PORT=9092
KAFKA_BROKER_EXTERNAL_PORT=29092

#mongodb configuration
MONGODB_INSTANCE_HOSTNAME=mongodb
MONGODB_INSTANCE_PORT=27017
MONGODB_CONNECTION_STRING=mongodb://mongodb:27017
----

This was transformed into a Kubernetes ConfigMap that centralizes all configuration:

.Kubernetes ConfigMap Configuration
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: ebikes-config
  namespace: ebikes
data:
  KAFKA_BROKER_HOSTNAME: "kafka-broker"
  KAFKA_BROKER_PORT: "9092"
  KAFKA_BROKER_EXTERNAL_PORT: "29092"
  MONGODB_INSTANCE_HOSTNAME: "mongodb"
  MONGODB_INSTANCE_PORT: "27017"
  MONGODB_CONNECTION_STRING: "mongodb://mongodb:27017"
  # ...additional configuration...
----

Microservices then reference this ConfigMap in their deployment manifests:

.Microservice ConfigMap Usage
[source,yaml]
----
containers:
  - name: ride-microservice
    image: microservices-ride-microservice:latest
    envFrom:
      - configMapRef:
          name: ebikes-config
----

=== Service Discovery and Networking

==== Kubernetes DNS Integration

In Docker Compose, services communicated using container names within the bridge network:

.Docker Compose Service Communication
[source,yaml]
----
services:
  eureka-server:
    # ...
    environment:
      - EUREKA_INSTANCE_HOSTNAME=eureka-server
  
  ride-microservice:
    # ...
    environment:
      - EUREKA_CLIENT_SERVICEURL_DEFAULTZONE=http://eureka-server:8761/eureka/
----

Kubernetes leverages its built-in DNS service discovery, where services are accessible via their Service names within the namespace:

.Kubernetes Service Discovery
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: eureka-server
  namespace: ebikes
spec:
  selector:
    app: eureka-server
  ports:
    - port: 8761
      targetPort: 8761
----

Services automatically resolve to `eureka-server.ebikes.svc.cluster.local`, but within the same namespace, the short name `eureka-server` works directly.

=== Dependency Management

==== From depends_on to Init Containers

Docker Compose used `depends_on` with health checks to manage service startup order:

.Docker Compose Dependencies
[source,yaml]
----
ride-microservice:
  # ...
  depends_on:
    eureka-server:
      condition: service_healthy
    ebike-microservice:
      condition: service_healthy
    user-microservice:
      condition: service_healthy
----

Kubernetes uses init containers to ensure dependencies are ready before starting the main container:

.Kubernetes Init Containers
[source,yaml]
----
spec:
  initContainers:
    - name: wait-for-eureka
      image: busybox
      command: ['sh', '-c', 'until nc -z eureka-server 8761; do echo waiting for eureka; sleep 2; done;']
    - name: wait-for-ebike
      image: busybox
      command: ['sh', '-c', 'until nc -z ebike-microservice 8080; do echo waiting for ebike-microservice; sleep 2; done;']
    - name: wait-for-user
      image: busybox
      command: ['sh', '-c', 'until nc -z user-microservice 8080; do echo waiting for user-microservice; sleep 2; done;']
  containers:
    - name: ride-microservice
      # ...main container configuration...
----

=== Kafka Infrastructure Transformation

==== Zookeeper and Kafka Broker Deployment

The Docker Compose Kafka setup was split into separate Kubernetes deployments for better resource management:

.Kubernetes Zookeeper Deployment
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper-1
  namespace: ebikes
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    spec:
      containers:
      - name: zookeeper
        image: confluentinc/cp-zookeeper:7.3.2
        env:
        - name: ZOOKEEPER_CLIENT_PORT
          value: "2181"
        - name: ZOOKEEPER_SERVER_ID
          value: "1"
----

.Kubernetes Kafka Broker with ConfigMap Integration
[source,yaml]
----
containers:
  - name: kafka
    image: confluentinc/cp-kafka:7.3.2
    env:
      - name: KAFKA_BROKER_HOSTNAME
        valueFrom:
          configMapKeyRef:
            name: ebikes-config
            key: KAFKA_BROKER_HOSTNAME
      - name: KAFKA_ADVERTISED_LISTENERS
        value: "PLAINTEXT://$(KAFKA_BROKER_HOSTNAME):29092,PLAINTEXT_HOST://$(KAFKA_BROKER_HOSTNAME):9092"
----

=== Storage Management

==== Persistent Volume Claims

MongoDB's data persistence was transformed from Docker volumes to Kubernetes PersistentVolumeClaims:

.Docker Compose Volume
[source,yaml]
----
mongodb:
  # ...
  volumes:
    - mongodb_data:/data/db

volumes:
  mongodb_data:
----

.Kubernetes Persistent Storage
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
  namespace: ebikes
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
# In the MongoDB deployment:
volumeMounts:
  - name: mongodb-data
    mountPath: /data/db
volumes:
  - name: mongodb-data
    persistentVolumeClaim:
      claimName: mongodb-pvc
----

=== Load Balancing and External Access

==== Service Types for External Access

Services requiring external access use LoadBalancer type:

.External Service Access
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: api-gateway
  namespace: ebikes
spec:
  selector:
    app: api-gateway
  ports:
    - port: 8080
      targetPort: 8080
  type: LoadBalancer  # Enables external access
----

=== Deployment Orchestration

==== Namespace Organization

All resources are organized within a dedicated namespace:

.Namespace Definition
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: ebikes
----

==== Deployment Script

The deployment process is automated through a script that applies all manifests:

.Kubernetes Deployment Script
[source,bash]
----
#!/bin/bash
set -e  # Exit on error

echo "Applying namespace..."
kubectl apply -f namespace.yml

echo "Applying Kubernetes manifests to 'ebikes' namespace..."
kubectl apply -f . --namespace=ebikes

echo "✅ Deployment complete."
----

=== Key Benefits of Kubernetes Transformation

Kubernetes transformation brings significant benefits. By leveraging replica sets, each microservice can be scaled independently to meet varying load patterns, ensuring reliable performance under heavy traffic. At the same time, Kubernetes resource management capabilities—such as defining CPU and memory requests and limits—provide fine-grained control over how cluster resources are allocated, leading to more efficient utilization and predictable application behavior. 

Configuration management is also greatly simplified: all environment-specific settings and sensitive data can be centralized in ConfigMaps and Secrets.

=== Event Driven Architecture Preservation

The event-driven architecture using Kafka topics remained unchanged during the transformation.

The transformation focused purely on the deployment and orchestration layer while preserving the application's core event driven architecture functionality and inter-service communication patterns.
